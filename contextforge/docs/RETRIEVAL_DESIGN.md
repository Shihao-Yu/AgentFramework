# Retrieval Design

This document describes the hybrid search architecture used in ContextForge for retrieving relevant knowledge nodes from the PostgreSQL database.

## Overview

ContextForge implements a hybrid search system that combines keyword-based search (BM25) with semantic vector search to provide robust retrieval capabilities. The system uses Reciprocal Rank Fusion (RRF) to merge results from both approaches.

## Hybrid Search Architecture

The retrieval system combines two complementary search methods:

### BM25 Full-Text Search (40% weight)
- PostgreSQL native full-text search using `tsvector` data type
- Keyword matching with term frequency and document frequency scoring
- Excellent for exact term matches and domain-specific terminology
- Fast execution with GIN index support

### Vector Similarity Search (60% weight)
- pgvector extension for cosine similarity computation
- 1024-dimensional embeddings capture semantic meaning
- Handles synonyms, paraphrasing, and conceptual similarity
- IVFFlat index for approximate nearest neighbor search

### Reciprocal Rank Fusion (RRF)
- Combines rankings from both methods without score normalization
- Formula: `RRF_score = Î£(1 / (k + rank_i))` where k=60
- Reduces impact of outlier scores from either method
- Produces stable, interpretable combined rankings

## The hybrid_search_nodes() Function

The core retrieval function is implemented as a PostgreSQL stored procedure:

```sql
CREATE OR REPLACE FUNCTION hybrid_search_nodes(
    query_text TEXT,
    query_embedding vector(1024),
    tenant_ids TEXT[],
    node_types TEXT[],
    top_k INTEGER DEFAULT 20,
    bm25_weight FLOAT DEFAULT 0.4,
    vector_weight FLOAT DEFAULT 0.6
) RETURNS TABLE (
    node_id INTEGER,
    bm25_score FLOAT,
    vector_score FLOAT,
    combined_score FLOAT
)
```

### Parameters

- `query_text`: Natural language query for BM25 search
- `query_embedding`: 1024-dim vector representation of the query
- `tenant_ids`: Array of tenant IDs to filter results
- `node_types`: Array of node types (e.g., ['field', 'qa_example'])
- `top_k`: Number of results to return (default: 20)
- `bm25_weight`: Weight for keyword search component (default: 0.4)
- `vector_weight`: Weight for semantic search component (default: 0.6)

### Return Values

Each result includes:
- `node_id`: Primary key of the knowledge node
- `bm25_score`: Raw BM25 relevance score
- `vector_score`: Cosine similarity score (0-1)
- `combined_score`: Weighted fusion score

## Retrieval Strategies

### Direct Query
Filter-based retrieval without ranking:
```python
nodes = adapter.get_nodes(
    tenant_id="acme_corp",
    dataset_name="sales_db",
    node_type="field"
)
```

Use cases:
- Browsing all nodes in a dataset
- Retrieving specific node types
- Administrative operations

### Similarity Search
Pure vector-based retrieval:
```python
similar_nodes = adapter.get_similar_nodes(
    query_embedding=embedding,
    tenant_id="acme_corp",
    top_k=10,
    similarity_threshold=0.7
)
```

Use cases:
- Finding conceptually similar content
- Semantic deduplication
- Exploratory search

### Hybrid Search
Combined keyword and semantic search:
```python
results = adapter.hybrid_search(
    query_text="customer email address",
    query_embedding=embedding,
    tenant_ids=["acme_corp"],
    node_types=["field"],
    top_k=20,
    bm25_weight=0.4,
    vector_weight=0.6
)
```

Use cases:
- Natural language queries
- Schema field matching
- Q&A example retrieval

## Score Interpretation

Understanding combined scores helps determine result quality:

| Score Range | Interpretation | Action |
|-------------|----------------|--------|
| 0.85+ | High confidence match | Use directly in generation |
| 0.70-0.84 | Good match | Include in results |
| 0.50-0.69 | Partial match | Review before using |
| <0.50 | Low confidence | Exclude from results |

### Score Components

**BM25 Score**
- Unbounded positive values
- Higher scores indicate better keyword matches
- Affected by term frequency and document length

**Vector Score**
- Range: -1 to 1 (cosine similarity)
- Typical useful range: 0.5 to 1.0
- Values below 0.5 indicate weak semantic similarity

**Combined Score**
- Weighted average of normalized component scores
- Range: 0 to 1
- Balances keyword precision with semantic recall

## Embedding Generation

Embeddings are generated by concatenating node attributes:

```python
text = f"{title} {summary} {content_description}"
embedding = embedding_model.encode(text)  # 1024-dim vector
```

### Text Preparation
- Title: Short identifier (e.g., field name, question)
- Summary: Brief description of the node
- Content description: Detailed information about the node

### Embedding Model
- Dimensionality: 1024
- Model: Configurable (default: sentence-transformers)
- Normalization: L2 normalized for cosine similarity

## Index Structure

Efficient retrieval requires proper indexing:

### Full-Text Search Index
```sql
CREATE INDEX idx_knowledge_nodes_searchable 
ON knowledge_nodes 
USING GIN(searchable_text);
```

- Type: GIN (Generalized Inverted Index)
- Column: `searchable_text` (tsvector)
- Purpose: Fast BM25 keyword search

### Vector Search Index
```sql
CREATE INDEX idx_knowledge_nodes_embedding 
ON knowledge_nodes 
USING ivfflat(embedding vector_cosine_ops)
WITH (lists = 100);
```

- Type: IVFFlat (Inverted File with Flat compression)
- Column: `embedding` (vector(1024))
- Purpose: Approximate nearest neighbor search
- Lists parameter: Number of clusters (tune based on dataset size)

### Filter Indexes
```sql
CREATE INDEX idx_knowledge_nodes_tenant 
ON knowledge_nodes(tenant_id);

CREATE INDEX idx_knowledge_nodes_type 
ON knowledge_nodes(node_type);

CREATE INDEX idx_knowledge_nodes_dataset 
ON knowledge_nodes(dataset_name);
```

- Type: B-tree
- Purpose: Fast filtering before search execution

## Tuning Parameters

### Weight Adjustment

**Increase BM25 weight (0.5-0.6) when:**
- Queries contain specific technical terms
- Exact keyword matching is critical
- Domain has specialized vocabulary
- Users provide structured queries

**Increase vector weight (0.7-0.8) when:**
- Queries are conversational or vague
- Semantic understanding is important
- Handling synonyms and paraphrasing
- Cross-domain concept matching

### Top-K Selection

- **Small datasets (<1000 nodes)**: top_k = 10-20
- **Medium datasets (1000-10000 nodes)**: top_k = 20-50
- **Large datasets (>10000 nodes)**: top_k = 50-100

Higher top_k values:
- Increase recall but may reduce precision
- Slower query execution
- More results to post-process

### Similarity Threshold

Set minimum combined_score for inclusion:
- **Strict**: 0.75+ (high precision, lower recall)
- **Balanced**: 0.60+ (good precision/recall tradeoff)
- **Permissive**: 0.50+ (high recall, lower precision)

## Performance Characteristics

### BM25 Search Complexity
- Time: O(log n) with GIN index
- Space: O(unique_terms)
- Scales well with document count
- Fast for keyword-heavy queries

### Vector Search Complexity
- Time: O(sqrt(n)) with IVFFlat index
- Space: O(n * dimensions)
- Approximate results (trade accuracy for speed)
- Slower than BM25 but handles semantic queries

### Combined Hybrid Search
- Parallel execution of both methods
- Merge at application layer using RRF
- Total time: max(BM25_time, vector_time) + merge_time
- Merge time is negligible (O(k log k))

### Optimization Tips

1. **Index Maintenance**: Run VACUUM and ANALYZE regularly
2. **IVFFlat Lists**: Set to sqrt(total_rows) for optimal performance
3. **Connection Pooling**: Reuse database connections
4. **Batch Queries**: Retrieve multiple node types in single call
5. **Caching**: Cache frequently accessed embeddings

## KnowledgeVerseAdapter Integration

The adapter provides high-level methods that use hybrid search internally:

### Schema Field Retrieval
```python
fields = adapter.get_similar_fields(
    query="customer email",
    tenant_id="acme_corp",
    dataset_name="sales_db",
    top_k=10
)
```

Returns `FieldSpec` objects compatible with QueryForge:
- field_name: Database column name
- field_type: SQL data type
- description: Human-readable description
- sample_values: Example values from the field

### Q&A Example Retrieval
```python
examples = adapter.get_similar_qa_examples(
    query="show total revenue",
    tenant_id="acme_corp",
    dataset_name="sales_db",
    top_k=5
)
```

Returns `ExampleSpec` objects for few-shot learning:
- question: Natural language query
- sql: Corresponding SQL query
- explanation: Why this SQL answers the question

### Conversion Pipeline

1. Execute hybrid_search_nodes() with appropriate filters
2. Fetch full node records from knowledge_nodes table
3. Parse metadata JSON into structured objects
4. Convert to QueryForge-compatible specs
5. Return sorted by combined_score

## Future Enhancements

### Potential Improvements

1. **Dynamic Weight Adjustment**: Learn optimal weights per query type
2. **Query Expansion**: Add synonyms and related terms automatically
3. **Re-ranking**: Apply neural re-ranker on top-k results
4. **Feedback Loop**: Use click-through data to improve rankings
5. **Multi-vector Search**: Separate embeddings for different node attributes

### Monitoring Metrics

Track these metrics to evaluate retrieval quality:
- Mean Reciprocal Rank (MRR)
- Precision@K and Recall@K
- Query latency (p50, p95, p99)
- Cache hit rate
- Index size and growth rate

## References

- PostgreSQL Full-Text Search: https://www.postgresql.org/docs/current/textsearch.html
- pgvector Documentation: https://github.com/pgvector/pgvector
- BM25 Algorithm: Robertson & Zaragoza (2009)
- Reciprocal Rank Fusion: Cormack et al. (2009)
